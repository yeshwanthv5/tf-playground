{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Tensorflow 2.0 \n",
    "\n",
    "Import Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get datasets.\n",
    "`tf.keras.datasets` has a few well known datasets. This downloads the dataset and `load_data` function gives the dataset. This loads the whole data onto the memory. So might not work with large datsets. For large datasets use `tfrecords`.\n",
    "\n",
    "* boston_housing\n",
    "* cifar10\n",
    "* cifar100\n",
    "* fashion_mnist\n",
    "* imdb\n",
    "* mnist\n",
    "* reuters\n",
    "\n",
    "Ref: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((50000, 32, 32, 3), (50000, 1)), ((10000, 32, 32, 3), (10000, 1)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "(x_train.shape, y_train.shape), (x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `tf.data` to create input data pipeline.\n",
    "The method `from_tensor_slices` reads the data from a tensor object, `shuffle` takes in the buffer size as argument and randomly samples one `batch` and replaces the next batch to the buffer. For example if we give `1000` as buffer size and `32` as batch size - a batch of `32` examples is randomly sampled from the first `1000` examples in the dataset and this batch is replaced by `1001-1032`th examples.\n",
    "\n",
    "Ref: \n",
    "* https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data\n",
    "* https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<BatchDataset shapes: ((None, 32, 32, 3), (None, 1)), types: (tf.float64, tf.uint8)>,\n",
       " <BatchDataset shapes: ((None, 32, 32, 3), (None, 1)), types: (tf.float64, tf.int64)>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "train_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build our model.\n",
    "The model class should have a method `call` which takes in input and defines the forward pass.\n",
    "In the constructor we define the componenet layers that are being used.\n",
    "\n",
    "Ref: https://www.tensorflow.org/guide/keras#model_subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MyModel at 0x1073e6390>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__(name = \"my_model\")\n",
    "        self.conv1 = Conv2D(32, 5, activation = \"relu\")\n",
    "        self.conv2 = Conv2D(32, 3, activation = \"relu\")\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Dense(128, activation = \"relu\")\n",
    "        self.fc2 = Dense(10, activation = \"softmax\")\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        return self.fc2(x)\n",
    "    \n",
    "model = MyModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create loss and optimizer object.\n",
    "\n",
    "Ref:\n",
    "* https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n",
    "* https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers/Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.losses.SparseCategoricalCrossentropy at 0x129d73da0>,\n",
       " <tensorflow.python.keras.optimizer_v2.adam.Adam at 0x129d73d68>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer_object = tf.keras.optimizers.Adam()\n",
    "loss_object, optimizer_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define metrics to monitor model training.\n",
    "We define loss as the average loss for all the batches in a given epoch.\n",
    "Accuracy metric is self-explainatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tensorflow.python.keras.metrics.Mean at 0x129da01d0>,\n",
       " <tensorflow.python.keras.metrics.Mean at 0x129da0160>,\n",
       " <tensorflow.python.keras.metrics.SparseCategoricalAccuracy at 0x129da0438>,\n",
       " <tensorflow.python.keras.metrics.SparseCategoricalAccuracy at 0x129d73828>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name = \"train_loss\")\n",
    "test_loss = tf.keras.metrics.Mean(name = \"test_loss\")\n",
    "train_acc = tf.keras.metrics.SparseCategoricalAccuracy(name = \"train_acc\")\n",
    "test_acc = tf.keras.metrics.SparseCategoricalAccuracy(name = \"test_acc\")\n",
    "train_loss, test_loss, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer_object.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_acc(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    loss = loss_object(labels, predictions)\n",
    "    \n",
    "    test_loss(loss)\n",
    "    test_acc(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together we train the training and test loop for 5 epochs and output the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 1.4760560989379883, Train Accuracy: 0.47266000509262085, Test Loss: 1.266222596168518, Test Accuracy: 0.550000011920929\n",
      "Epoch: 2, Train Loss: 1.3055193424224854, Train Accuracy: 0.5345900058746338, Test Loss: 1.2107973098754883, Test Accuracy: 0.5748000144958496\n",
      "Epoch: 3, Train Loss: 1.1809097528457642, Train Accuracy: 0.5804866552352905, Test Loss: 1.2025797367095947, Test Accuracy: 0.5842666625976562\n",
      "Epoch: 4, Train Loss: 1.075687289237976, Train Accuracy: 0.6184849739074707, Test Loss: 1.235257625579834, Test Accuracy: 0.5885249972343445\n",
      "Epoch: 5, Train Loss: 0.9841580986976624, Train Accuracy: 0.6512519717216492, Test Loss: 1.2955611944198608, Test Accuracy: 0.5892999768257141\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "    for images, labels in test_ds:\n",
    "        test_step(images, labels)\n",
    "    template = \"Epoch: {}, Train Loss: {}, Train Accuracy: {}, Test Loss: {}, Test Accuracy: {}\"\n",
    "    print(template.format(epoch + 1, train_loss.result(), train_acc.result(),\n",
    "                         test_loss.result(), test_acc.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is poor for the likes of CIFAR-10. We need a more sophisticated model to get better accuracy on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
